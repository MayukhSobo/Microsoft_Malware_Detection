{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we shall be performing all the required n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from copy import deepcopy\n",
    "from textwrap import wrap\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Process, Manager\n",
    "from sklearn.externals import joblib\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz, vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASM  BYTE  sample  trainLabels.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASMPATH = \"./dataset/ASM/\"\n",
    "BYTEPATH = \"./dataset/BYTE/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10868"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path, dirs, asmfiles = next(os.walk(ASMPATH))\n",
    "len(asmfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10868"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path, dirs, bytefiles = next(os.walk(BYTEPATH))\n",
    "len(bytefiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_feature_names(n):\n",
    "    feature_names = []\n",
    "    max_number = 'F' * (2 * n)\n",
    "    max_number_dec = int(max_number, 16)\n",
    "    for i in range(max_number_dec + 1):\n",
    "        feat_str = str(hex(i)).split('x')[1]\n",
    "        feat_str = feat_str.upper()\n",
    "        feature_names.append(feat_str.rjust(2*n, '0'))\n",
    "    feature_names.append('other')\n",
    "    return dict(zip(feature_names, [0] * len(feature_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-gram counts of byte files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(worker, n, bytefiles, one_gram_counts):\n",
    "    features = get_ngram_feature_names(n)\n",
    "    for byte in bytefiles:\n",
    "        with open(BYTEPATH + byte, 'r') as fin:\n",
    "            content = fin.readlines()\n",
    "        counts = deepcopy(features)\n",
    "        for line in content:\n",
    "            line = line[9:]\n",
    "            ng = map(lambda x: \"\".join(x), ngrams(line.split(), n))\n",
    "            for gram in ng:\n",
    "                try:\n",
    "                    counts[gram] += 1\n",
    "                except KeyError:\n",
    "                    counts['other'] += 1\n",
    "        counts['fileId'] = byte.split('.')[0]\n",
    "        one_gram_counts.append(counts)\n",
    "        print(f'Done for {byte}')\n",
    "    print(f'Worker = {worker} is completed')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    manager = Manager()\n",
    "    one_gram_counts = manager.list()\n",
    "    jobs = []\n",
    "    for i in range(18):\n",
    "        p = Process(target=get_ngrams, args=(i, 1, bytefiles[i*605:(i+1)*605], one_gram_counts,))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "    for proc in jobs:\n",
    "        proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(one_gram_counts) == len(bytefiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one_gram_counts.pkl']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(list(one_gram_counts), 'one_gram_counts.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bi-gram counts of byte files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker = 15 is completed\n",
      "Worker = 0 is completed\n",
      "Worker = 16 is completed\n",
      "Worker = 2 is completed\n",
      "Worker = 1 is completed\n",
      "Worker = 17 is completed\n",
      "Worker = 3 is completed\n",
      "Worker = 6 is completed\n",
      "Worker = 4 is completed\n",
      "Worker = 14 is completed\n",
      "Worker = 13 is completed\n",
      "Worker = 9 is completed\n",
      "Worker = 10 is completed\n",
      "Worker = 7 is completed\n",
      "Worker = 8 is completed\n",
      "Worker = 5 is completed\n",
      "Worker = 12 is completed\n",
      "Worker = 11 is completed\n"
     ]
    }
   ],
   "source": [
    "def get_ngrams(worker, n, bytefiles, bigram_counts):\n",
    "    features = get_ngram_feature_names(n)\n",
    "    for byte in bytefiles:\n",
    "        counts = deepcopy(features)\n",
    "        with open(BYTEPATH + byte, 'r') as fin:\n",
    "#             content = fin.readlines()\n",
    "            for line in fin:\n",
    "                line = line[9:]\n",
    "                ng = map(lambda x: \"\".join(x), ngrams(line.split(), n))\n",
    "                for gram in ng:\n",
    "                    try:\n",
    "                        counts[gram] += 1\n",
    "                    except KeyError:\n",
    "                        counts['other'] += 1\n",
    "        counts['fileId'] = byte.split('.')[0]\n",
    "        bigram_counts.append(counts)\n",
    "#         print(f'Done for the byte file {byte}')\n",
    "    print('Worker = {} is completed'.format(worker))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    manager = Manager()\n",
    "    bigram_counts = manager.list()\n",
    "    jobs = []\n",
    "    for i in range(18):\n",
    "        p = Process(target=get_ngrams, args=(i, 2, bf[i*302:(i+1)*302], bigram_counts,))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "    for proc in jobs:\n",
    "        proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_npz('bigram.npz')\n",
    "y = joblib.load('ngram_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10868, 65537)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10868,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = load_npz('monogram.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10868, 257)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASM opcode monogram and bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for the 4\n",
      "Done for the 10\n",
      "Done for the 5\n",
      "Done for the 7\n",
      "Done for the 15\n",
      "Done for the 1\n",
      "Done for the 0\n",
      "Done for the 2\n",
      "Done for the 17\n",
      "Done for the 9\n",
      "Done for the 13\n",
      "Done for the 16\n",
      "Done for the 8\n",
      "Done for the 11\n",
      "Done for the 12\n",
      "Done for the 3\n",
      "Done for the 6\n",
      "Done for the 14\n"
     ]
    }
   ],
   "source": [
    "opex = re.compile(r'(   )([a-z]+)( )')\n",
    "def get_opcode_strings(worker, asmfiles, all_opcodes):\n",
    "    for asm in asmfiles:\n",
    "        with open(ASMPATH + asm, 'r', encoding='ISO-8859-1') as fin:\n",
    "            opcodes = []\n",
    "            for line in fin:\n",
    "                e = opex.search(line)\n",
    "                if e:\n",
    "                    opcodes.append(e.group(2))\n",
    "            s = \" \".join(opcodes)\n",
    "            all_opcodes.append([s, asm.split('.')[0]])\n",
    "    print(f'Done for the {worker}')\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    manager = Manager()\n",
    "    all_opcodes = manager.list()\n",
    "    jobs = []\n",
    "    for i in range(18):\n",
    "        p = Process(target=get_opcode_strings, args=(i, asmfiles[i*605:(i+1)*605], all_opcodes,))\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "    for proc in jobs:\n",
    "        proc.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10868/10868 [00:11<00:00, 968.46it/s] \n"
     ]
    }
   ],
   "source": [
    "real_opcodes = []\n",
    "fileIds = []\n",
    "for opcode in tqdm(all_opcodes):\n",
    "    real_opcodes.append(opcode[0])\n",
    "    fileIds.append(opcode[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['opcode_fileIds']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(fileIds, 'opcode_fileIds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cunt = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "monogram_opcode = cunt.fit_transform(real_opcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('monogram_opcode.npz', monogram_opcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cunt = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_opcode = cunt.fit_transform(real_opcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('bigram_opcode.npz', bigram_opcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10868, 26327)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_opcode.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
